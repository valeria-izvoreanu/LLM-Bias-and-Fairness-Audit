{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-31T11:40:35.109673Z",
     "iopub.status.busy": "2025-05-31T11:40:35.109415Z",
     "iopub.status.idle": "2025-05-31T11:41:26.608489Z",
     "shell.execute_reply": "2025-05-31T11:41:26.607606Z",
     "shell.execute_reply.started": "2025-05-31T11:40:35.109643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets scikit-learn\n",
    "#!conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n",
    "#!pip install -U \"transformers>=4.41.0\" \"peft>=0.15.0\"\n",
    "#!pip install hf_xet\n",
    "!pip install fairlearn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.stats import mode\n",
    "import random\n",
    "from fairlearn.metrics import selection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:26.609913Z",
     "iopub.status.busy": "2025-05-31T11:41:26.609429Z",
     "iopub.status.idle": "2025-05-31T11:41:26.623668Z",
     "shell.execute_reply": "2025-05-31T11:41:26.622892Z",
     "shell.execute_reply.started": "2025-05-31T11:41:26.609888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fixing randomness for reproduceability\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:26.625960Z",
     "iopub.status.busy": "2025-05-31T11:41:26.625718Z",
     "iopub.status.idle": "2025-05-31T11:41:27.038027Z",
     "shell.execute_reply": "2025-05-31T11:41:27.037404Z",
     "shell.execute_reply.started": "2025-05-31T11:41:26.625943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/ethics-amazon-review/dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.039027Z",
     "iopub.status.busy": "2025-05-31T11:41:27.038789Z",
     "iopub.status.idle": "2025-05-31T11:41:27.043699Z",
     "shell.execute_reply": "2025-05-31T11:41:27.042739Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.039008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def keep_only_language(df, language):\n",
    "    \"\"\"\n",
    "    Keeps only rows where the 'language' column matches the specified language.\n",
    "    \"\"\"\n",
    "    return df[df['language'] == language].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.045038Z",
     "iopub.status.busy": "2025-05-31T11:41:27.044711Z",
     "iopub.status.idle": "2025-05-31T11:41:27.066588Z",
     "shell.execute_reply": "2025-05-31T11:41:27.066042Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.044998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_and_check_nulls(df, name=\"Dataset\"):\n",
    "    \"\"\"Function that cleans the dataset from nulls\"\"\"\n",
    "    original_len = len(df)\n",
    "    df = df.dropna(subset=['review_title'])\n",
    "    dropped = original_len - len(df)\n",
    "    print(f\"{name}: Dropped {dropped} rows with null 'review_title'.\")\n",
    "    \n",
    "    remaining_nulls = df.isnull().sum()\n",
    "    if remaining_nulls.sum() == 0:\n",
    "        print(f\"{name}: No remaining nulls.\")\n",
    "    else:\n",
    "        print(f\"{name}: Remaining null values:\\n{remaining_nulls[remaining_nulls > 0]}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.067747Z",
     "iopub.status.busy": "2025-05-31T11:41:27.067285Z",
     "iopub.status.idle": "2025-05-31T11:41:27.122997Z",
     "shell.execute_reply": "2025-05-31T11:41:27.122454Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.067729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = clean_and_check_nulls(test_df, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.123959Z",
     "iopub.status.busy": "2025-05-31T11:41:27.123722Z",
     "iopub.status.idle": "2025-05-31T11:41:27.130469Z",
     "shell.execute_reply": "2025-05-31T11:41:27.129620Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.123940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Function that drops columns that are not useful, and creates the sentiment column\n",
    "    Drops the languages that are not useful for our analysis\n",
    "    Furthermore, it plots the label distribution across languages, to ensure the dataset is balanced, from a data point number point of view\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe.drop(columns=[\"Unnamed: 0\",\"review_id\", \"product_id\", \"reviewer_id\"], axis = 1, inplace = True)\n",
    "\n",
    "    dataframe = dataframe[dataframe['language'].isin(['de', 'es', 'en'])]\n",
    "\n",
    "    dataframe = dataframe[dataframe['stars'] != 3]\n",
    "    \n",
    "    dataframe['sentiment'] = dataframe['stars'].apply(lambda x: 0 if x in [1, 2] else 1)\n",
    "\n",
    "    sentiment_counts = dataframe.groupby(['language', 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    sentiment_counts.plot(kind='bar', stacked=False)\n",
    "    plt.title('Sentiment Distribution per Language')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.legend(title='Sentiment (0=Negative, 1=Positive)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    dataframe.drop(columns=[\"stars\"], axis = 1, inplace = True)\n",
    "\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.131520Z",
     "iopub.status.busy": "2025-05-31T11:41:27.131262Z",
     "iopub.status.idle": "2025-05-31T11:41:27.587430Z",
     "shell.execute_reply": "2025-05-31T11:41:27.586868Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.131494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"/nProcessing TEST dataset\")\n",
    "test_df = process_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.589636Z",
     "iopub.status.busy": "2025-05-31T11:41:27.589425Z",
     "iopub.status.idle": "2025-05-31T11:41:27.593438Z",
     "shell.execute_reply": "2025-05-31T11:41:27.592666Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.589620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function_test(example):\n",
    "    \"\"\"\n",
    "    Tokenization function used for evaluation on the test set\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.594543Z",
     "iopub.status.busy": "2025-05-31T11:41:27.594334Z",
     "iopub.status.idle": "2025-05-31T11:41:27.620690Z",
     "shell.execute_reply": "2025-05-31T11:41:27.620159Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.594528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt_templates = {\n",
    "        'es': \"CATEGORÍA: {category}\\nTÍTULO: {title}\\nRESEÑA: {body}\",\n",
    "    }\n",
    "\n",
    "def format_row(row, lang_code = None):\n",
    "    \"\"\"\n",
    "    Function that formats the input of the model\n",
    "    \"\"\"\n",
    "    lang = lang_code if lang_code else row['language']\n",
    "    template = prompt_templates.get(lang, prompt_templates['es'])\n",
    "    return template.format(\n",
    "        category=row['product_category'],\n",
    "        title=row['review_title'],\n",
    "        body=row['review_body']\n",
    "    )\n",
    "\n",
    "def format_convert_and_show(df, name=\"Dataset\", num_samples=1, lang_code=None):\n",
    "    \"\"\"Function applies the input formatting, and creates the input field for the model\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['text'] = df.apply(format_row, axis=1)\n",
    "\n",
    "    df = df.drop(columns=['review_title', 'review_body', 'product_category'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.621603Z",
     "iopub.status.busy": "2025-05-31T11:41:27.621423Z",
     "iopub.status.idle": "2025-05-31T11:41:27.702020Z",
     "shell.execute_reply": "2025-05-31T11:41:27.701466Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.621590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lang = \"es\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "test_lang_df = keep_only_language(test_df, lang)\n",
    "\n",
    "test_lang_df = format_convert_and_show(test_lang_df, name=f\"Test {lang}\", num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.703525Z",
     "iopub.status.busy": "2025-05-31T11:41:27.702993Z",
     "iopub.status.idle": "2025-05-31T11:41:27.710844Z",
     "shell.execute_reply": "2025-05-31T11:41:27.710038Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.703495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_fairness_metrics(y_true, y_pred, group_name=\"\"):\n",
    "    \"\"\"\n",
    "    Function used to compute the SAPMOC metrics\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # hyperparameter used in defensive code. Useful to avoid division by 0\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    statistical_parity = (tp + fp) / (tp + fp + tn + fn + epsilon)\n",
    "    equality_of_opportunity = tp / (tp + fn + epsilon)\n",
    "    calibration_pos = tp / (tp + fp + epsilon)\n",
    "    calibration_neg = tn / (tn + fn + epsilon)\n",
    "    conditional_use_error_pos = fp / (tp + fp + epsilon)\n",
    "    conditional_use_error_neg = fn / (tn + fn + epsilon)\n",
    "    treatment_equality_fp_fn = fp / (fn + epsilon) if fn != 0 else 0\n",
    "    treatment_equality_fn_fp = fn / (fp + epsilon) if fp != 0 else f0\n",
    "    \n",
    "    print(f\"\\nFairness Metrics ({group_name})\")\n",
    "    print(f\"Statistical Parity             : {statistical_parity:.4f}\")\n",
    "    print(f\"Equality of Opportunity        : {equality_of_opportunity:.4f}\")\n",
    "    print(f\"Calibration (Pos)              : {calibration_pos:.4f}\")\n",
    "    print(f\"Calibration (Neg)              : {calibration_neg:.4f}\")\n",
    "    print(f\"Conditional Use Error (P)      : {conditional_use_error_pos:.4f}\")\n",
    "    print(f\"Conditional Use Error (N)      : {conditional_use_error_neg:.4f}\")\n",
    "    print(f\"Treatment Equality (FP/FN)     : {treatment_equality_fp_fn:.4f}\")\n",
    "    print(f\"Treatment Equality (FN/FP)     : {treatment_equality_fn_fp:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"statistical_parity\": statistical_parity,\n",
    "        \"equality_of_opportunity\": equality_of_opportunity,\n",
    "        \"calibration_pos\": calibration_pos,\n",
    "        \"calibration_neg\": calibration_neg,\n",
    "        \"conditional_use_error_pos\": conditional_use_error_pos,\n",
    "        \"conditional_use_error_neg\": conditional_use_error_neg,\n",
    "        \"treatment_equality_fp_fn\": treatment_equality_fp_fn,\n",
    "        \"treatment_equality_fn_fp\": treatment_equality_fn_fp\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.711865Z",
     "iopub.status.busy": "2025-05-31T11:41:27.711575Z",
     "iopub.status.idle": "2025-05-31T11:41:27.734635Z",
     "shell.execute_reply": "2025-05-31T11:41:27.733898Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.711843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results_table = []\n",
    "\n",
    "def evaluate_and_record(model_id, model, tokenizer, test_df, device, lang_code=None):\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "    test_dataset = test_dataset.map(tokenize_function_test, batched=True)\n",
    "    test_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_id} ({lang_code or 'all'})\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred = torch.argmax(outputs.logits, dim=-1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    y_true = np.array(labels)\n",
    "    y_pred = np.array(preds)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(f\"Confusion Matrix: {model_id} ({lang_code or 'all'})\")\n",
    "    plt.show()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    sel_rate = selection_rate(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Selection Rate: {sel_rate:.4f}\")\n",
    "\n",
    "    fairness_metrics = compute_fairness_metrics(y_true, y_pred, group_name=f\"{model_id} ({lang_code or 'all'})\")\n",
    "\n",
    "    entry = {\n",
    "        \"model\": model_id,\n",
    "        \"lang\": lang_code or \"all\",\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"selection_rate\": sel_rate,\n",
    "        **fairness_metrics\n",
    "    }\n",
    "    results_table.append(entry)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.735584Z",
     "iopub.status.busy": "2025-05-31T11:41:27.735398Z",
     "iopub.status.idle": "2025-05-31T11:41:27.757851Z",
     "shell.execute_reply": "2025-05-31T11:41:27.757037Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.735570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for the general model.\n",
    "def format_convert_and_show(df, name=\"Dataset\", num_samples=0):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['text'] = df.apply(\n",
    "        lambda x: f\"CATEGORY: {x['product_category']}\\nTITLE: {x['review_title']}\\nREVIEW TEXT: {x['review_body']}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- {name} Sample Entries ---\\n\")\n",
    "    for i, row in df.head(num_samples).iterrows():\n",
    "        print(f\"Sample {i+1}\")\n",
    "        print(f\"Category: {row['product_category']}\")\n",
    "        print(f\"Title: {row['review_title']}\")\n",
    "        print(f\"Review: {row['review_body']}\")\n",
    "        print(f\"Full text:\\n{row['text']}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:41:27.758795Z",
     "iopub.status.busy": "2025-05-31T11:41:27.758608Z",
     "iopub.status.idle": "2025-05-31T11:47:58.590283Z",
     "shell.execute_reply": "2025-05-31T11:47:58.589639Z",
     "shell.execute_reply.started": "2025-05-31T11:41:27.758780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the general distilbert model finetuned on the full test set\n",
    "model_id, model_name, model_path = (\"distilbert_all\", \"distilbert-base-multilingual-cased\", \"/kaggle/input/distilbert-all-lang/best_model_all.pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "print(\"General DistilBERT - All Languages\")\n",
    "test_df = format_convert_and_show(test_df)\n",
    "evaluate_and_record(model_id, model, tokenizer, test_df.copy(), device)\n",
    "\n",
    "# Per-language distilbert evaluations\n",
    "for lang_code in [\"es\", \"de\", \"en\"]:\n",
    "    df_lang = keep_only_language(test_df, lang_code)\n",
    "    df_lang = format_convert_and_show(df_lang)\n",
    "    print(f\"\\nGeneral DistilBERT - Language: {lang_code}\")\n",
    "    evaluate_and_record(model_id, model, tokenizer, df_lang, device, lang_code=lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:47:58.591341Z",
     "iopub.status.busy": "2025-05-31T11:47:58.591062Z",
     "iopub.status.idle": "2025-05-31T11:47:58.596824Z",
     "shell.execute_reply": "2025-05-31T11:47:58.596164Z",
     "shell.execute_reply.started": "2025-05-31T11:47:58.591322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_convert_and_show(df, name=\"Dataset\", num_samples=2, lang_code=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    prompt_templates = {\n",
    "        'es': \"CATEGORÍA: {category}\\nTÍTULO: {title}\\nRESEÑA: {body}\",\n",
    "    }\n",
    "\n",
    "    def format_row(row):\n",
    "        lang = lang_code if lang_code else row['language']\n",
    "        template = prompt_templates.get(lang, prompt_templates['es'])\n",
    "        return template.format(\n",
    "            category=row['product_category'],\n",
    "            title=row['review_title'],\n",
    "            body=row['review_body']\n",
    "        )\n",
    "\n",
    "    df['text'] = df.apply(format_row, axis=1)\n",
    "\n",
    "    df = df.drop(columns=['review_title', 'review_body', 'product_category'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:47:58.597913Z",
     "iopub.status.busy": "2025-05-31T11:47:58.597716Z",
     "iopub.status.idle": "2025-05-31T11:53:46.987523Z",
     "shell.execute_reply": "2025-05-31T11:53:46.986711Z",
     "shell.execute_reply.started": "2025-05-31T11:47:58.597897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spanish_models = [\n",
    "    (\"distilbert\", \"distilbert-base-multilingual-cased\", \"/kaggle/input/spanish-models/best_model_es_distilbert.pt\"),\n",
    "    (\"xlm_roberta\", \"xlm-roberta-base\", \"/kaggle/input/spanish-models/best_model_es_xlm_roberta.pt\"),\n",
    "    (\"bert\", \"bert-base-multilingual-cased\", \"/kaggle/input/spanish-models/best_model_es_bert.pt\")\n",
    "]\n",
    "\n",
    "spanish_test_df = keep_only_language(test_df, \"es\")\n",
    "spanish_test_df = format_convert_and_show(spanish_test_df, lang_code=\"es\")\n",
    "\n",
    "spanish_preds = []\n",
    "\n",
    "for model_id, model_name, model_path in spanish_models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    print(f\"\\n {model_id} - Spanish only\")\n",
    "    y_pred = evaluate_and_record(model_id, model, tokenizer, spanish_test_df, device, lang_code=\"es\")\n",
    "    spanish_preds.append(y_pred)\n",
    "\n",
    "# Ensemble model from Spanish-only\n",
    "print(\"\\nEnsemble Model - Spanish only\")\n",
    "stacked_preds = np.stack(spanish_preds, axis=0)\n",
    "majority_preds, _ = mode(stacked_preds, axis=0)\n",
    "majority_preds = majority_preds.squeeze()\n",
    "\n",
    "y_true = spanish_test_df[\"sentiment\"].values\n",
    "\n",
    "cm = confusion_matrix(y_true, majority_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix: Ensemble (Spanish)\")\n",
    "plt.show()\n",
    "\n",
    "fairness_metrics = compute_fairness_metrics(y_true, majority_preds, group_name=\"ensemble (es)\")\n",
    "\n",
    "results_table.append({\n",
    "    \"model\": \"ensemble\",\n",
    "    \"lang\": \"es\",\n",
    "    \"accuracy\": accuracy_score(y_true, majority_preds),\n",
    "    \"f1_score\": f1_score(y_true, majority_preds),\n",
    "    \"selection_rate\": selection_rate(y_true, majority_preds),\n",
    "    **fairness_metrics\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results_table)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:53:46.988706Z",
     "iopub.status.busy": "2025-05-31T11:53:46.988414Z",
     "iopub.status.idle": "2025-05-31T11:53:46.994156Z",
     "shell.execute_reply": "2025-05-31T11:53:46.993301Z",
     "shell.execute_reply.started": "2025-05-31T11:53:46.988677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_convert_and_show(df, name=\"Dataset\", num_samples=0):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['text'] = df.apply(\n",
    "        lambda x: f\"CATEGORY: {x['product_category']}\\nTITLE: {x['review_title']}\\nREVIEW TEXT: {x['review_body']}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- {name} Sample Entries ---\\n\")\n",
    "    for i, row in df.head(num_samples).iterrows():\n",
    "        print(f\"Sample {i+1}\")\n",
    "        print(f\"Category   : {row['product_category']}\")\n",
    "        print(f\"Title      : {row['review_title']}\")\n",
    "        print(f\"Review     : {row['review_body']}\")\n",
    "        print(f\"Full text  :\\n{row['text']}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-31T11:53:46.995153Z",
     "iopub.status.busy": "2025-05-31T11:53:46.994919Z",
     "iopub.status.idle": "2025-05-31T11:56:13.624075Z",
     "shell.execute_reply": "2025-05-31T11:56:13.623430Z",
     "shell.execute_reply.started": "2025-05-31T11:53:46.995111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the distilbert model finetunned on german and english, on their respective languages\n",
    "eval_models = [\n",
    "    (\"distilbert\", \"distilbert-base-multilingual-cased\", \"de\", \"/kaggle/input/models-en-de-best/best_model_de.pt\"),\n",
    "    (\"distilbert\", \"distilbert-base-multilingual-cased\", \"en\", \"/kaggle/input/models-en-de-best/best_model_en.pt\")\n",
    "]\n",
    "\n",
    "for model_id, model_name, lang_code, model_path in eval_models:\n",
    "    print(f\"\\n {model_id} - Fine-tuned on {lang_code.upper()} only\")\n",
    "\n",
    "    test_lang_df = keep_only_language(test_df, lang_code)\n",
    "    test_lang_df = format_convert_and_show(test_lang_df)\n",
    "    y_true = test_lang_df[\"sentiment\"].values\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    y_pred = evaluate_and_record(model_id, model, tokenizer, test_lang_df, device, lang_code=lang_code)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(f\"Confusion Matrix: {model_id.upper()} ({lang_code.upper()})\")\n",
    "    plt.show()\n",
    "\n",
    "    fairness_metrics = compute_fairness_metrics(y_true, y_pred, group_name=f\"{model_id} ({lang_code})\")\n",
    "\n",
    "    results_table.append({\n",
    "        \"model\": model_id,\n",
    "        \"lang\": lang_code,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"selection_rate\": selection_rate(y_true, y_pred),\n",
    "        **fairness_metrics\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_table)\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7492367,
     "sourceId": 11918006,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7521669,
     "sourceId": 11961932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7527049,
     "sourceId": 11969960,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7548186,
     "sourceId": 11999400,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
