{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11918006,"sourceType":"datasetVersion","datasetId":7492367}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers datasets scikit-learn\n#!conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n#!pip install -U \"transformers>=4.41.0\" \"peft>=0.15.0\"\n#!pip install hf_xet\n!pip install fairlearn\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom datasets import Dataset\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW \nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom IPython.display import FileLink\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom fairlearn.metrics import (\n    MetricFrame,\n    selection_rate,\n    equalized_odds_difference,\n    demographic_parity_difference,\n    false_positive_rate_difference,\n    false_negative_rate_difference,\n    true_positive_rate_difference,\n    true_negative_rate_difference\n)\n\nimport random\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:01:39.225611Z","iopub.execute_input":"2025-05-31T11:01:39.225818Z","execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/ethics-amazon-review/dataset/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/ethics-amazon-review/dataset/validation.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ethics-amazon-review/dataset/test.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"General dataset info:\")\nprint(f\"Training set: {train_df.info()}\")\nprint(f\"\\n Validation set: {val_df.info()}\")\nprint(f\"\\n Test set: {test_df.info()}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_and_check_nulls(df, name=\"Dataset\"):\n    \"\"\"Function that cleans the dataset from nulls\"\"\"\n    original_len = len(df)\n    df = df.dropna(subset=['review_title'])\n    dropped = original_len - len(df)\n    print(f\"{name}: Dropped {dropped} rows with null 'review_title'.\")\n    \n    remaining_nulls = df.isnull().sum()\n    if remaining_nulls.sum() == 0:\n        print(f\"{name}: No remaining nulls.\")\n    else:\n        print(f\"{name}: Remaining null values:\\n{remaining_nulls[remaining_nulls > 0]}\")\n    \n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = clean_and_check_nulls(train_df, \"Training Set\")\nval_df = clean_and_check_nulls(val_df, \"Validation Set\")\ntest_df = clean_and_check_nulls(test_df, \"Test Set\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_dataframe(dataframe):\n    \"\"\"Function that drops columns that are not useful, and creates the sentiment column\n    Drops the languages that are not useful for our analysis\n    Furthermore, it plots the label distribution across languages, to ensure the dataset is balanced, from a data point number point of view\"\"\"\n\n    \n    dataframe.drop(columns=[\"Unnamed: 0\",\"review_id\", \"product_id\", \"reviewer_id\"], axis = 1, inplace = True)\n    dataframe = dataframe[dataframe['language'].isin(['de', 'es', 'en'])]\n    \n\n    dataframe = dataframe[dataframe['stars'] != 3]\n    dataframe['sentiment'] = dataframe['stars'].apply(lambda x: 0 if x in [1, 2] else 1)\n\n    sentiment_counts = dataframe.groupby(['language', 'sentiment']).size().unstack(fill_value=0)\n    \n    sentiment_counts.plot(kind='bar', stacked=False)\n    plt.title('Sentiment Distribution per Language')\n    plt.xlabel('Language')\n    plt.ylabel('Number of Reviews')\n    plt.legend(title='Sentiment (0=Negative, 1=Positive)')\n    plt.tight_layout()\n    plt.show()\n\n    dataframe.drop(columns=[\"stars\"], axis = 1, inplace = True)\n\n    return dataframe\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Processing TRAIN dataset\")\ntrain_df = process_dataframe(train_df)\n\nprint(\"/nProcessing VALIDATION dataset\")\nval_df = process_dataframe(val_df)\n\nprint(\"/nProcessing TEST dataset\")\ntest_df = process_dataframe(test_df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#HYPARPARAMETERS\nMODEL_NAME = \"distilbert-base-multilingual-cased\"\nEPOCHS = 3\nLR = 5e-6 # 10 times lower than in the paper.\nbatch_size = 16","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def keep_only_language(df, language):\n    \"\"\"\n    Keeps only rows where the 'language' column matches the specified language.\n    \"\"\"\n    return df[df['language'] == language].reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_convert_and_show(df, name=\"Dataset\", num_samples=0):\n    df = df.copy()\n\n    df['text'] = df.apply(\n        lambda x: f\"CATEGORY: {x['product_category']}\\nTITLE: {x['review_title']}\\nREVIEW TEXT: {x['review_body']}\",\n        axis=1\n    )\n\n    print(f\"\\n--- {name} Sample Entries ---\\n\")\n    for i, row in df.head(num_samples).iterrows():\n        print(f\"Sample {i+1}\")\n        print(f\"Category   : {row['product_category']}\")\n        print(f\"Title      : {row['review_title']}\")\n        print(f\"Review     : {row['review_body']}\")\n        print(f\"Full text  :\\n{row['text']}\\n\")\n        print(\"-\" * 50)\n\n    df = df.drop(columns=['review_title', 'review_body', 'product_category'], axis = 1)\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = format_convert_and_show(train_df, \"Training Set\")\nval_dataset = format_convert_and_show(val_df, \"Validation Set\")\ntest_dataset = format_convert_and_show(test_df, \"Test Set\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n\ndef tokenize_function(example):\n    \"\"\"\n    Tokenization function used for the training and validation loop\n    \"\"\"\n    tokenized = tokenizer(example['text'], truncation=True)\n    return tokenized\n\ndef tokenize_function_test(example):\n    \"\"\"\n    Tokenization function used for the testing loop\n    \"\"\"\n    tokenized = tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=512)\n    return tokenized","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_dataset.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_dataset.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_dataset.reset_index(drop=True))\n\nprint(test_dataset.shape)\nprint(test_dataset.column_names)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function_test, batched=True)\n\ntrain_dataset = train_dataset.rename_column(\"sentiment\", \"labels\")\nval_dataset = val_dataset.rename_column(\"sentiment\", \"labels\")\ntest_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format(\n    type='torch',\n    columns=['input_ids', 'attention_mask', 'labels', 'language'],\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=data_collator)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, scheduler, device, save_path, epochs=5):\n\n    \"\"\"\n    Training loop function\n    \"\"\"\n    \n    best_f1 = 0.0\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=True)\n\n        for batch in loop:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n            loop.set_postfix(loss=loss.item())\n\n        print(f\"\\nEpoch {epoch + 1} - Training loss: {total_loss / len(train_loader):.4f}\")\n\n        acc, f1 = evaluate_validation_model(model, val_loader, device, return_scores=True)\n        scheduler.step(f1)\n\n        if f1 > best_f1:\n            best_f1 = f1\n            torch.save(model.state_dict(), save_path)\n            print(f\"Best model saved with F1: {f1:.4f}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_validation_model(model, val_loader, device, return_scores=False):\n    \"\"\"\n    Function used in the validation phase\n    returns the desired metrics for our training\n    \"\"\"\n    all_preds, all_labels = [], []\n\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=-1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n\n    if return_scores:\n        return acc, f1\n    else:\n        return all_labels, all_preds","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, test_loader):\n\n    all_preds, all_labels, group_labels = [], [], []\n\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            languages = batch['language']\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=-1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            group_labels.extend(languages)\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(\"Test Accuracy:\", acc)\n    print(\"Test F1 Score:\", f1)\n    print(\"Confusion Matrix:\\n\", cm)\n\n    frame = MetricFrame(\n        metrics={\n            \"accuracy\": accuracy_score,\n            \"f1\": f1_score,\n            \"selection_rate\": selection_rate,\n        },\n        y_true=all_labels,\n        y_pred=all_preds,\n        sensitive_features=group_labels\n    )\n\n    print(\"\\n=== Metrics by Language Group ===\")\n    print(frame.by_group)\n\n    return all_labels, all_preds\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def download_weights(file_path):\n    \"\"\"\n    Function used to create a downloadable link to download the resulting weights.\n    \"\"\"\n    if os.path.exists(file_path):\n        display(FileLink(file_path))\n    else:\n        print(f\"File {file_path} does not exist.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finetuning the general DistilBert model on all 3 languages\nprint(\"Defining model...\")\n        \nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n                                                       num_labels=2,\n                                                      ).to(device)\noptimizer = AdamW(model.parameters(), lr=LR)\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n\nprint(\"Freezing model...\")\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\n\nprint(f\"\\nTraining for language: all\")\nsave_path = f\"./best_model_all.pt\"\ntrain_model(model, train_loader, val_loader, optimizer, scheduler, device, save_path, epochs=EPOCHS)\ntest_model(model, test_loader)\ndownload_weights(save_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_templates = {\n    'en': \"CATEGORY: {category}\\nTITLE: {title}\\nREVIEW TEXT: {body}\",\n    'de': \"KATEGORIE: {category}\\nTITEL: {title}\\nBEWERTUNGSTEXT: {body}\",\n    'es': \"CATEGORÍA: {category}\\nTÍTULO: {title}\\nRESEÑA: {body}\",\n}\ndef format_row(row, lang_code = None):\n    \"\"\"\n    Function used to dynamically adapt the training prompt for each language.\n    \"\"\"\n    lang = lang_code if lang_code else row['language']\n    template = prompt_templates.get(lang, prompt_templates['en'])\n    return template.format(\n        category=row['product_category'],\n        title=row['review_title'],\n        body=row['review_body']\n    )\n\ndef format_convert_and_show(df, name=\"Dataset\", num_samples=2, lang_code=None):\n    df = df.copy()\n\n    df['text'] = df.apply(format_row, axis=1)\n\n    df = df.drop(columns=['review_title', 'review_body', 'product_category'], axis=1)\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finetuning DistilBert on each language in particular, and evaluating it\nfor lang in ['de', 'en', 'es']:\n    \n    train_lang_df = keep_only_language(train_df, lang)\n    val_lang_df = keep_only_language(val_df, lang)\n    test_lang_df = keep_only_language(test_df, lang)\n\n    train_lang_df = format_convert_and_show(train_lang_df, name=f\"Train {lang}\", num_samples=0)\n    val_lang_df = format_convert_and_show(val_lang_df, name=f\"Val {lang}\", num_samples=0)\n    test_lang_df = format_convert_and_show(test_lang_df, name=f\"Test {lang}\", num_samples=0)\n\n    train_dataset = Dataset.from_pandas(train_lang_df.reset_index(drop=True))\n    val_dataset = Dataset.from_pandas(val_lang_df.reset_index(drop=True))\n    test_dataset = Dataset.from_pandas(test_lang_df.reset_index(drop=True))\n\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    val_dataset = val_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function_test, batched=True)\n\n    train_dataset = train_dataset.rename_column(\"sentiment\", \"labels\")\n    val_dataset = val_dataset.rename_column(\"sentiment\", \"labels\")\n    test_dataset = test_dataset.rename_column(\"sentiment\", \"labels\")\n\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    test_dataset.set_format(\n        type='torch',\n        columns=['input_ids', 'attention_mask', 'labels', 'language'],\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=data_collator)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    print(\"Defining model...\")\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n                                                           num_labels=2,\n                                                          ).to(device)\n    optimizer = AdamW(model.parameters(), lr=LR)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n\n    print(\"Freezing model...\")\n    for param in model.distilbert.parameters():\n        param.requires_grad = False\n    \n    \n    \n    print(f\"\\nTraining for language: {lang}\")\n    save_path = f\"./best_model_{lang}.pt\"\n    train_model(model, train_loader, val_loader, optimizer, scheduler, device, save_path, epochs=EPOCHS)\n    test_model(model, test_loader)\n    download_weights(save_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:01:57.009Z"}},"outputs":[],"execution_count":null}]}